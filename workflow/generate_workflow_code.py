import os
import yaml
import litellm
import json 
from pathlib import Path

litellm.model = "xai/grok-3-latest"  # or any supported model

PROMPT_DIR = "llm_prompts"

def auto_generate_prompt(name, description):
    # Use GPT to generate a clean system prompt for the step
    messages = [
        {"role": "system", "content": "You are an expert prompt writer for LLM function agents."},
        {"role": "user", "content": f"""
Write a system prompt for an LLM agent named `{name}`.

Its goal is:
{description}

This LLM will receive input as a dictionary (e.g. `data`) and should return a modified `data` dictionary.

Your response must include:
- The agentâ€™s role
- Detailed instruction for what it should do
- What the input looks like
- What the expected output looks like
"""}
    ]
    response = litellm.completion(model=litellm.model, messages=messages)
    return response["choices"][0]["message"]["content"].strip()

def build_prompts(workflow_path: str, save: bool = True):
    with open(workflow_path, "r") as f:
        workflow = yaml.safe_load(f)

    prompts = {}
    os.makedirs(PROMPT_DIR, exist_ok=True)

    for step in workflow["steps"]:
        name = step["step"]
        desc = step["description"]
        prompt_path = f"{PROMPT_DIR}/{name}.yaml"

        if os.path.exists(prompt_path):
            print(f"ğŸ“„ Loading existing prompt: {prompt_path}")
            with open(prompt_path, "r") as f:
                prompts[name] = yaml.safe_load(f)
        else:
            print(f"âœ¨ Generating new system prompt for: {name}")
            content = auto_generate_prompt(name, desc)
            prompt_obj = {
                "name": name,
                "role": "system",
                "content": content
            }
            prompts[name] = prompt_obj

            if save:
                with open(prompt_path, "w") as f:
                    yaml.dump(prompt_obj, f, allow_unicode=True)
                print(f"âœ… Saved new prompt: {prompt_path}")

    return prompts

def run_llm_step(prompt: dict, user_input: dict):
    messages = [
        {"role": prompt["role"], "content": prompt["content"]},
        {"role": "user", "content": f"{user_input}"}
    ]
    response = litellm.completion(model=litellm.model, messages=messages)
    return response["choices"][0]["message"]["content"].strip()

def run_workflow(workflow_path: str, initial_data: dict, prompts: dict):
    with open(workflow_path, "r") as f:
        workflow = yaml.safe_load(f)

    data = initial_data

    for step in workflow["steps"]:
        name = step["step"]
        prompt = prompts[name]

        print(f"\nâš™ï¸ Running step: {name}")
        result = run_llm_step(prompt, data)
        print(f"ğŸ” Output: {result}")

        try:
            parsed = json.loads(result)
            if isinstance(parsed, dict):
                data.update(parsed)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parse error in step {name}: {e}")
            print(f"ğŸ” Raw output: {result}")

    return data

if __name__ == "__main__":
    workflow_file = "presentation_workflow.yaml"
    workflow_file = "image_search_workflow.yaml"

    # # 1. Generate system prompts from workflow (optionally save)
    # prompts = build_prompts(workflow_file, save=True)

    # # 2. Initial input for the first step
    # initial_input = {
    #     "speaker_notes": "åŸå­ç»“æ„çš„æ„æˆæ˜¯ç”±åŸå­æ ¸ä¸æ ¸å¤–ç”µå­æ„æˆçš„ï¼Œè€ŒåŸå­æ ¸å¤–ç”µå­çš„å‘ç°æ˜¯åœ¨1897å¹´ï¼Œç”±æ±¤å§†é€Šçš„å‡ºè‰²å®éªŒå¾—åˆ°çš„ã€‚æ±¤å§†é€Šçš„å®éªŒè¿‡ç¨‹æ˜¯è¿™æ ·çš„ï¼Œä»–å°†ä¸€å—æ¶‚æœ‰ç¡«åŒ–é”Œçš„å°ç»ç’ƒç‰‡ï¼Œæ”¾åœ¨é˜´æå°„çº¿æ‰€ç»è¿‡çš„è·¯é€”ä¸Šï¼Œçœ‹åˆ°ç¡«åŒ–é”Œä¼šå‘é—ªå…‰ã€‚è¿™è¯´æ˜ç¡«åŒ–é”Œèƒ½æ˜¾ç¤ºå‡ºé˜´æå°„çº¿çš„â€œå¾„è¿¹â€ã€‚ä»–å‘ç°åœ¨ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œé˜´æå°„çº¿æ˜¯ç›´çº¿è¡Œè¿›çš„ï¼Œä½†å½“åœ¨å°„å‡»çº¿ç®¡çš„å¤–é¢åŠ ä¸Šç”µåœºï¼Œæˆ–ç”¨ä¸€å—è¹„å½¢ç£é“è·¨æ”¾åœ¨å°„çº¿ç®¡çš„å¤–é¢ï¼Œç»“æœå‘ç°é˜´æå°„çº¿ä¸€éƒ½å‘ç”Ÿäº†åæŠ˜ã€‚æ ¹æ®å…¶åæŠ˜çš„æ–¹å‘ï¼Œä¸éš¾åˆ¤æ–­å‡ºå¸¦ç”µçš„æ€§è´¨ã€‚æ±¤å§†é€Šåœ¨1897å¹´å¾—å‡ºç»“è®ºï¼šè¿™äº›â€œå°„çº¿â€ä¸æ˜¯ä»¥å¤ªæ³¢ï¼Œè€Œæ˜¯å¸¦è´Ÿç”µçš„ç‰©è´¨ç²’å­ã€‚ä½†ä»–åé—®è‡ªå·²ï¼šâ€œè¿™äº›ç²’å­æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿå®ƒä»¬æ˜¯åŸå­è¿˜æ˜¯åˆ†å­ï¼Œè¿˜æ˜¯å¤„åœ¨æ›´ç»†çš„å¹³è¡¡çŠ¶æ€ä¸­çš„ç‰©è´¨ï¼Ÿâ€œè¿™éœ€è¦ä½œæ›´ç²¾ç»†çš„å®éªŒï¼Œå½“æ—¶è¿˜ä¸çŸ¥é“æ¯”åŸå­æ›´å°çš„ä¸œè¥¿ï¼Œå› æ­¤æ±¤å§†é€Šå‡å®šè¿™æ˜¯ä¸€ç§è¢«ç”µç¦»çš„åŸå­ï¼Œå³å¸¦è´Ÿç”µçš„â€œç¦»å­â€ã€‚ä»–è¦æµ‹é‡å‡ºè¿™ç§â€œç¦»å­â€çš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œä»–è®¾è®¡äº†ä¸€ç³»åˆ—å³ç®€å•åˆå·§å¦™çš„å®éªŒï¼šé¦–å…ˆï¼Œè®¾è®¡å•ç‹¬çš„ç”µåœºæˆ–ç£åœºéƒ½èƒ½ä½¿å¸¦ç”µä½“åè½¬ï¼Œè€Œç£åœºå¯¹ç²’å­æ–½åŠ çš„åŠ›æ˜¯ä¸ç²’å­çš„é€Ÿåº¦æœ‰å…³çš„ã€‚æ±¤å§†é€Šå¯¹ç²’å­åŒæ—¶æ–½åŠ ä¸€ä¸ªç”µåœºå’Œç£åœºï¼Œå¹¶è°ƒèŠ‚åˆ°ç”µåœºå’Œç£åœºæ‰€é€ æˆçš„ç²’å­çš„åè½¬äº’ç›¸æŠµæ¶ˆï¼Œè®©ç²’å­ä»ä½œç›´çº¿è¿åŠ¨ã€‚è¿™æ ·ï¼Œä»ç”µåœºå’Œç£åœºçš„å¼ºåº¦æ¯”å€¼å°±èƒ½ç®—å‡ºç²’å­è¿åŠ¨é€Ÿåº¦ã€‚è€Œé€Ÿåº¦ä¸€æ—¦æ‰¾åˆ°åï¼Œå•é ç£åè½¬æˆ–è€…ç”µåè½¬å°±å¯ä»¥æµ‹å‡ºç²’å­çš„ç”µè·ä¸è´¨é‡çš„æ¯”å€¼ã€‚æ±¤å§†é€Šç”¨è¿™ç§æ–¹æ³•æ¥æµ‹å®šâ€œå¾®ç²’â€ç”µè·ä¸è´¨é‡ä¹‹æ¯”å€¼ã€‚ä»–å‘ç°è¿™ä¸ªæ¯”å€¼å’Œæ°”ä½“çš„æ€§è´¨æ— å…³ï¼Œå¹¶ä¸”è¯¥å€¼æ¯”èµ·ç”µè§£è´¨ä¸­æ°¢ç¦»å­çš„æ¯”å€¼ï¼ˆè¿™æ˜¯å½“æ—¶å·²çŸ¥çš„æœ€å¤§é‡ï¼‰è¿˜è¦å¤§å¾—å¤šï¼Œè¿™è¯´æ˜è¿™ç§ç²’å­çš„è´¨é‡æ¯”æ°¢åŸå­çš„è´¨é‡è¦å°å¾—å¤šã€‚å‰è€…å¤§çº¦æ˜¯åè€…çš„äºŒåƒåˆ†ä¹‹ä¸€ã€‚"
    # }

    # # 3. Run the entire LLM workflow
    # final_output = run_workflow(workflow_file, initial_input, prompts)

    # print("\nğŸ‰ Final Output:\n", final_output)

    slides_file = "slides.json"  # Update path if needed

    # === STEP 1: Load Slides JSON ===
    with open(slides_file, "r", encoding="utf-8") as f:
        slide_data = json.load(f)
        slides = slide_data["slides"]


    # === STEP 2: Build Prompts ===
    prompts = build_prompts(workflow_file, save=True)

    # === STEP 3: Run Workflow for Each Slide ===
    all_outputs = []
    for idx, slide in enumerate(slides):
        speaker_notes = slide.get("script") or slide.get("speaker_notes") or ""
        if not speaker_notes.strip():
            print(f"âš ï¸ Skipping empty script at index {idx}")
            continue

        print(f"\nğŸš€ Running workflow for Slide {idx+1}...")

        input_data = {"speaker_notes": speaker_notes}
        result = run_workflow(workflow_file, input_data, prompts)
        
        # Optionally attach the result to the slide
        slide["generated"] = result
        all_outputs.append(result)

    # === STEP 4: Save Updated Slides (optional) ===
    output_path = Path(slides_file).with_name("slides_with_images.json")
    slide_data["slides"] = slides
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(slide_data, f, ensure_ascii=False, indent=2)

    print("\nâœ… All slides processed. Output saved to:", output_path)
